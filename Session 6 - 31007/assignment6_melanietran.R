dataPath <- setwd('/Users/meltra02/Desktop/MSCA/Quarter 1/31007 - Stats Analysis/Session 6 - 31007')
train_dat <- read.table(paste(dataPath,'Week6_Test_Sample_Train.csv',sep = '/'), header=TRUE)
main_dat <- read.table(paste(dataPath,'Week6_Test_Sample_Test.csv',sep = '/'), header=TRUE)

nSample.Training<-length(train_dat[,1])
head(train_dat)

#Plot the training sample
plot(train_dat[,2],train_dat[,1], type="p",pch=19)

# Define training samples generated by model 1 and model 2
train_dat.1<-cbind(train_dat[,2],rep(NA,nSample.Training))
train_dat.2<-cbind(train_dat[,2],rep(NA,nSample.Training))
train_dat.1[train_dat[,3]*(1:nSample.Training),2]<-
  train_dat[train_dat[,3]*(1:nSample.Training),2]
train_dat.2[(1-train_dat[,3])*(1:nSample.Training),2]<-
  train_dat[(1-train_dat[,3])*(1:nSample.Training),2]

head(cbind(train_dat,
           Training1=train_dat.1[,2],
           Training2=train_dat.2[,2]))

# Plot the subsamples
matplot(train_dat[,1],cbind(train_dat.1[,2],train_dat.2[,2]),
        pch=16,col=c("green","blue"),ylab="Subsamples of the training sample")

#Estimate linear model for the training sample <- SKIPPED CODE MT
EstimatedLinearModel.Training<- lm(train_dat[,1] ~ train_dat[,2], train_dat)
summary(EstimatedLinearModel.Training)$coefficients

#calculate r squared
summary(EstimatedLinearModel.Training)$r.squared

#calculate sd
summary(EstimatedLinearModel.Training)$sigma

#compare the results for the training sample and main sample for R^2 and sigma
EstimatedResiduals.Training<-EstimatedLinearModel.Training$residuals
#if this was one population, you would find more density around zero
#you find some pockets and density further away from zero 
#this is because there is a sub population above and below 0

plot(train_dat[,2],EstimatedResiduals.Training)

# Define residuals corresponding to different models 
EstimatedResiduals.Training.1<-EstimatedResiduals.Training
EstimatedResiduals.Training.2<-EstimatedResiduals.Training
EstimatedResiduals.Training.1[(train_dat[,3]==0)*(1:nSample.Training)]<-NA
EstimatedResiduals.Training.2[(train_dat[,3]==1)*(1:nSample.Training)]<-NA

# Print the first ten columns to check the separation 
head(cbind(AllResiduals=EstimatedResiduals.Training,
           Training1Residuals=EstimatedResiduals.Training.1,
           Training2Residuals=EstimatedResiduals.Training.2,
           TrainingClass=train_dat[,3]))

# Plot the residuals corresponding to different models
matplot(train_dat[,2],cbind(EstimatedResiduals.Training.1,
                                       EstimatedResiduals.Training.2),
        pch=16,col=c("green","blue"),ylab="Separated parts of the training sample")


#1.2 Logistic Regression 
# Create the data frame for logistic regression
Logistic.Model.Data<-data.frame(Logistic.Output=train_dat[,3],
                                Logistic.Input=EstimatedResiduals.Training)
train_dat.Logistic<-glm(Logistic.Output~Logistic.Input,data=Logistic.Model.Data,
                                   family=binomial(link=logit))
summary(train_dat.Logistic)

names(train_dat.Logistic)

#Define and plot probability of selecting an observation from the first model using predict function with type="response" argument. 
#Plot the predicted probabilities
Predicted.Probabilities.Training<-predict(train_dat.Logistic,type="response")
#when the predicted probabilities are close to 0 or 1, those data points let you know if it is closer to ex.female (1) or male (1)
#you want to look at the points in the middle are the "uncertainty" region
#your model can't fully distinguish if it is male or female 
#you want your model to be more definitive ie closer to 1 or 0 
plot(train_dat[,2],Predicted.Probabilities.Training)

#Classify the training sample using the estimated unscrambling sequence
# Create the unscrambling sequence for the training sample
Unscrambling.Sequence.Training.Logistic<-
  (predict(train_dat.Logistic,type="response")>.5)*1

# Create classified residuals
ClassifiedResiduals.Training.1<-EstimatedResiduals.Training
ClassifiedResiduals.Training.2<-EstimatedResiduals.Training
ClassifiedResiduals.Training.1[(Unscrambling.Sequence.Training.Logistic==0)*
                                 (1:nSample.Training)]<-NA
ClassifiedResiduals.Training.2[(Unscrambling.Sequence.Training.Logistic==1)*
                                 (1:nSample.Training)]<-NA
head(cbind(AllTraining=EstimatedResiduals.Training,
           Training1=ClassifiedResiduals.Training.1,
           Training2=ClassifiedResiduals.Training.2))

# Plot both classes of the residuals
matplot(train_dat[,2],cbind(ClassifiedResiduals.Training.1,
                                       ClassifiedResiduals.Training.2),
        pch=16,col=c("green","blue"),ylab="Classified residuals, X-axis at 0")
axis(1,pos=0)

#MT - SKIPPED CODE
#Calculate classification boundary for the models using estimated coefficients of logistic regression.
#is based on p(>0.5) 
#gives you a new boundary that says is the residual > or < the classification boundary 

Classification.Rule.Logistic <- (-train_dat.Logistic$coefficients[1]/train_dat.Logistic$coefficients[2])



##GOOD UNTIL HERE##

##MAIN##
#1.3 Separate subsamples 
main_dat <- read.table(paste(dataPath,'Week6_Test_Sample_Test.csv',sep = '/'), header=TRUE)

nSampleMain<-length(main_dat[,1])
head(main_dat)

#Estimate linear model from the main sample and find the residuals.
MainEstimatedLinearModel<-lm(main_dat[,1]~main_dat[,2])
MainEstimatedLinearModel$coefficients

MainEstimatedResiduals<-MainEstimatedLinearModel$residuals
plot(main_dat[,2],MainEstimatedResiduals)

#Define the predicted probabilities and separating sequence 
Unscrambling.Sequence.Logistic<-(predict(train_dat.Logistic,newdata=data.frame(Logistic.Output=MainEstimatedResiduals,Logistic.Input=MainEstimatedResiduals),
                                         type="response")>.5)*1
head(Unscrambling.Sequence.Logistic)

#estimate probability of the data classified as the first model
Probability<-sum(Unscrambling.Sequence.Logistic)/length(Unscrambling.Sequence.Logistic)
Probability

#conduct binom test - SKIPPED CODE
binom.test(541,n = nSampleMain)


#Classify residuals of the main sample into 2 groups 

# Create classified residuals
ClassifiedResiduals.1<-MainEstimatedResiduals
ClassifiedResiduals.2<-MainEstimatedResiduals
ClassifiedResiduals.1[(Unscrambling.Sequence.Logistic==0)*(1:nSampleMain)]<-NA
ClassifiedResiduals.2[(Unscrambling.Sequence.Logistic==1)*(1:nSampleMain)]<-NA
# Print first 10 rows to check
cbind(MainEstimatedResiduals,ClassifiedResiduals.1,ClassifiedResiduals.2)[1:10,]

# Plot both classes of the residuals
matplot(main_dat[,2],cbind(ClassifiedResiduals.1,
                              ClassifiedResiduals.2),
        pch=16,col=c("green","blue"),ylab="Classes of the main sample, X-axis at 0")
axis(1,pos=0)

#The X axis on the plot above is located at zero level. The following graph shows it at the level of the classification rule estimated by logistic model.
matplot(main_dat[,2],cbind(ClassifiedResiduals.1,ClassifiedResiduals.2),
        pch=16,col=c("green","blue"),ylab="Classes of the main sample, X-axis at the rule level")
axis(1,pos=Classification.Rule.Logistic)

#Separate the given sample into 2 subsamples using the trained logistic model.
# Create recovered models
main_dat1.Recovered<-main_dat
main_dat2.Recovered<-main_dat
main_dat1.Recovered[(1-Unscrambling.Sequence.Logistic)*(1:nSampleMain),2]<-NA
main_dat2.Recovered[Unscrambling.Sequence.Logistic*(1:nSampleMain),2]<-NA
# Print the first 1 rows of scrambled and unscrambled samples
cbind(main_dat,main_dat1.Recovered,main_dat2.Recovered)[1:10,]

# Plot the unscrambled subsamples
matplot(main_dat[,2],cbind(main_dat1.Recovered[,1],main_dat2.Recovered[,1]), type="p",col=c("green","blue"),pch=19,ylab="Separated Subsamples")


#ANSWER
res <- list(Unscrambling.Sequence.Logistic =  Unscrambling.Sequence.Logistic)
write.table(res, file = paste(dataPath,'result.csv',sep = '/'), row.names = F)
